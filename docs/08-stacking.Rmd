---
title: "Model Stacking & AutoML"
author: "Brad Boehmke"
date: "2019-03-01"
output:
  xaringan::moon_reader:
    css: ["scrollable.css", "mtheme_max.css", "fonts_mtheme_max.css"]
    self_contained: false
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false  
---

```{r setup, include=FALSE, cache=FALSE}
# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center", 
  cache = FALSE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  eval = FALSE
)

library(tidyverse)
# set ggplot to black and white theme
library(ggplot2)
theme_set(theme_bw())
```

class: clear, center, middle

background-image: url(images/stacking-icon.jpg)
background-position: center
background-size: cover

<br><br><br>
.font300.white[Model Stacking & AutoML]


---
# Introduction

.pull-left[

.center.bold.font120[Thoughts]

- Original concept formalized by Leo Breiman [`r anicon::aia("google-scholar", animate = 'tada', anitype="hover")`](http://statistics.berkeley.edu/sites/default/files/tech-reports/367.pdf)

- Theoretically formalized in 2007 as ___Super Learners___ [`r anicon::aia("google-scholar", animate = 'tada', anitype="hover")`](https://www.degruyter.com/view/j/sagmb.2007.6.issue-1/sagmb.2007.6.1.1309/sagmb.2007.6.1.1309.xml) where the authors...

- proved that super learners will learn the optimal combination of supplied base learners and will typically perform as well as or better than any of the individual base learners.

- Nearly all prediction competitions are won with super learners

]

--

.pull-right[

.center.bold.font120[Overview]

- Basic idea

- Stacking existing models

- Stacking a grid search

- Auto machine learning search

]


---
# Prereqs .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 1]

.scrollable90[
.pull-left[

.center.bold.font120[Packages]

```{r prereqs-pks}
library(recipes)
library(h2o)
h2o.init(max_mem_size = "5g")
```

```{r no-progress, echo=FALSE}
h2o.no_progress()
```

]

.pull-right[

.center.bold.font120[Data]

```{r prereqs-data}
# ames data
ames <- AmesHousing::make_ames()

# split data
set.seed(123)
split <- rsample::initial_split(ames, strata = "Sale_Price")
ames_train <- rsample::training(split)
ames_test <- rsample::testing(split)

# make sure we have consistent categorical levels
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_other(all_nominal(), threshold = .005)

# create training & test sets
train_h2o <- prep(blueprint, training = ames_train, retain = TRUE) %>%
  juice() %>%
  as.h2o()

test_h2o <- prep(blueprint, training = ames_train) %>%
  bake(new_data = ames_test) %>%
  as.h2o()

# get names of response and features
Y <- "Sale_Price"
X <- setdiff(names(ames_train), Y)
```
]
]

---
class: clear, center, middle, inverse

.font300.white[Basic Idea]


---
# Common ensemble methods

<br>
.font110[
* Ensemble machine learning methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms.

* Combining multiple predictors is not new
   - Bagging
   - Random forests
   - Gradient boosting
   
* However, these ensemble approaches combine common weak base learning algorithms (i.e. decision trees)  

* Stacking, on the other hand, is designed to .bold.blue[ensemble a diverse group of strong learners.]
]

---
# Super learner algorithm

Part 1: Set up the ensemble
- Specify a list of _L_ base learners (with a specific set of model parameters).
- Specify a metalearning algorithm. Can be any one of the algorithms discussed in the previous chapters but most often is regularized regression.

---
# Super learner algorithm

.opacity[Part 1: Set up the ensemble]

Part 2: Train the ensemble
- Train each of the _L_ base learners on the training set.
- Perform k-fold cross-validation on each of these learners and collect the cross-validated predicted values from each of the _L_ algorithms (must use the same k-folds for each base learner). These predicted values represent  $p_1,…,p_L$.
- The _N_ cross-validated predicted values from each of the _L_ algorithms can be combined to form a new $N \times L$ matrix (represented by _Z_). This matrix, along with the original response vector (_y_), is called the “level-one” data. (N = number of rows in the training set.)

```{r stacking-eq, echo=FALSE}
knitr::include_graphics("images/stacking-equation.png")
```

- Train the metalearning algorithm on the level-one data ( $y=f(Z)$ ). The “ensemble model” consists of the __L__ base learning models and the metalearning model, which can then be used to generate predictions on a test set.

---
# Super learner algorithm

.opacity[Part 1: Set up the ensemble]

.opacity[Part 2: Train the ensemble]

Part 3: Predict on new data
- To generate ensemble predictions, first generate predictions from the base learners.
- Feed those predictions into the metalearner to generate the ensemble prediction.

<br><br><br><br>
--

.center.bold.font90[_Stacking rarely does worse than selecting the single best base learner on the training data. .blue[The biggest gains are usually produced when stacking base learners that have high variability, and uncorrelated, predicted values.] The more similar the predicted values, the less advantage there is in stacking._]

---
# Package implementation `r emo::ji("package")`
<br>
.font110[
* [h2o](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html): My go-to package for stacking and autoML. Provides three appraoches for model stacking.

* [SuperLearner](https://github.com/ecpolley/SuperLearner): original implementation, works with `caret` and many other algorithm packages.  Worth exploring.

* [subsemble](https://github.com/ledell/subsemble): developed by Erin Ledell who now is one of the developers of __h2o__. Maintained for backward compatibility but not forward development

* [caretEnsemble](https://github.com/zachmayer/caretEnsemble): implements a boostrapped (rather than cross-validated) version of stacking. The bootstrapped version will train faster since bootrapping (with a train/test) is a fraction of the work as k-fold cross-validation, however the the ensemble performance suffers as a result of this shortcut.
]

---
class: clear, center, middle, inverse

.font300.white[Stacking Existing Models]  
.white[_Train first, stack later_]

---
# Stacking existing models

.pull-left.font90[
Say we found the optimal hyperparameters that provided the best predictive accuracy for a:

1. Regularized regression base learner
2. Random forest base learner
3. Stochastic GBM base learner
4. XGBoost base learner

To stack them later we need to do a few specific things:

1. All models must be trained on the same training set.
2. All models must be trained with the same number of CV folds.
3. All models must use the same fold assignment to ensure the same observations are used (`fold_assignment = "Modulo"`).
4. The cross-validated predictions from all of the models must be preserved (`keep_cross_validation_predictions = True`).

]

---
# Stacking existing models

.scrollable90[
.pull-left.font90[
Say we found the optimal hyperparameters that provided the best predictive accuracy for a:

1. Regularized regression base learner
2. Random forest base learner
3. Stochastic GBM base learner
4. XGBoost base learner

To stack them later we need to do a few specific things:

1. All models must be trained on the same training set.
2. All models must be trained with the same number of CV folds.
3. All models must use the same fold assignment to ensure the same observations are used (`fold_assignment = "Modulo"`).
4. The cross-validated predictions from all of the models must be preserved (`keep_cross_validation_predictions = True`).

]

.pull-right[

.center.bold.font90[`r anicon::faa("exclamation-triangle", color = "red", animate = FALSE)`  This code takes ~12 min `r anicon::faa("exclamation-triangle", color = "red", animate = FALSE)`]

```{r existing-models, eval=FALSE}
# Train & Cross-validate a GLM model
best_glm <- h2o.glm(
  x = X,
  y = Y,
  training_frame = train_h2o,
  alpha = .1,
  remove_collinear_columns = TRUE,
  nfolds = 10, #<<
  fold_assignment = "Modulo", #<<
  keep_cross_validation_predictions = TRUE, #<<
  seed = 123
  )

h2o.rmse(best_glm, xval = TRUE)
## [1] 35638.96

# Train & Cross-validate a RF model
best_rf <- h2o.randomForest(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 1000,
  mtries = 20,
  max_depth = 30,
  min_rows = 1,
  sample_rate = 0.8,
  nfolds = 10, #<<
  fold_assignment = "Modulo", #<<
  keep_cross_validation_predictions = TRUE, #<<
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
  )

h2o.rmse(best_rf, xval = TRUE)
## [1] 24103.8

# Train & Cross-validate a GBM model
best_gbm <- h2o.gbm(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 5000,
  learn_rate = 0.01,
  max_depth = 7,
  min_rows = 5,
  sample_rate = 0.8,
  nfolds = 10, #<<
  fold_assignment = "Modulo", #<<
  keep_cross_validation_predictions = TRUE, #<<
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
  )

h2o.rmse(best_gbm, xval = TRUE)
## [1] 21747.52

# Train & Cross-validate an XGBoost model
best_xgb <- h2o.xgboost(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 5000,
  learn_rate = 0.05,
  max_depth = 3,
  min_rows = 3,
  sample_rate = 0.8,
  categorical_encoding = "Enum",
  nfolds = 10, #<<
  fold_assignment = "Modulo", #<<
  keep_cross_validation_predictions = TRUE, #<<
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
)

h2o.rmse(best_xgb, xval = TRUE)
## [1] 20936.64
```
]
]

---
# Stacking existing models

.pull-left[

* use `h2o.stackedEnsemble()` to stack these models

* we can use many different metalearning algorithms ("superlearners")
   - `glm`: regularized linear regression
   - `drf`: random forest
   - `gbm`: gradient boosted machine
   - `deeplearning`: neural network

]

.pull-right[

```{r stacking-existing-mods, eval=FALSE}
# Train a stacked tree ensemble
ensemble_tree <- h2o.stackedEnsemble(
  x = X,
  y = Y,
  training_frame = train_h2o,
  model_id = "my_tree_ensemble",
  base_models = list(best_glm, best_rf, best_gbm, best_xgb),
  metalearner_algorithm = "drf" #<<
  )
```

]

---
# Stacking existing models

.scrollable90[
.pull-left[

* use `h2o.stackedEnsemble()` to stack these models

* we can use many different metalearning algorithms ("superlearners")
   - `glm`: regularized linear regression
   - `drf`: random forest
   - `gbm`: gradient boosted machine
   - `deeplearning`: neural network
   
* results illustrate a slight improvement

* we're restricted on how much improvement stacking will make due to highly correlated predictions


]

.pull-right[

```{r stacking-existing-mods-results, eval=FALSE}
# base learners
get_rmse <- function(model) {
  results <- h2o.performance(model, newdata = test_h2o)
  results@metrics$RMSE
}

list(best_glm, best_rf, best_gbm, best_xgb) %>%
  purrr::map_dbl(get_rmse)
## [1] 30024.67 23075.24 20859.92 21391.20

# stacked glm
results_tree <- h2o.performance(ensemble_tree, newdata = test_h2o)
results_tree@metrics$RMSE
## [1] 20664.56
```

```{r prediction-correlation, eval=FALSE}
data.frame(
  GLM_pred = as.vector(h2o.getFrame(best_glm@model$cross_validation_holdout_predictions_frame_id$name)),
  RF_pred = as.vector(h2o.getFrame(best_rf@model$cross_validation_holdout_predictions_frame_id$name)),
  GBM_pred = as.vector(h2o.getFrame(best_gbm@model$cross_validation_holdout_predictions_frame_id$name)),
  XGB_pred = as.vector(h2o.getFrame(best_xgb@model$cross_validation_holdout_predictions_frame_id$name))
  ) %>%
  cor()
##           GLM_pred   RF_pred  GBM_pred  XGB_pred
## GLM_pred 1.0000000 0.9390229 0.9291982 0.9345048
## RF_pred  0.9390229 1.0000000 0.9920349 0.9821944
## GBM_pred 0.9291982 0.9920349 1.0000000 0.9854160
## XGB_pred 0.9345048 0.9821944 0.9854160 1.0000000
```

]
]

---
class: clear, center, middle, inverse

.font300.white[Stacking a Grid Search]

---
# Stacking a grid search

.scrollable90[
.pull-left[
* We can also stack multiple models generated from the same base learner

* Certain tuning parameters allow us to find unique patterns within the data

* By stacking the results of a grid search, we can capitalize on the benefits of each of the models in our grid search to create a meta model

* For example, the following performs a random grid search across a wide range of hyperparameter settings. We set the search to stop after 25 models have run.
]

.pull-right[

```{r gbm-grid-search, eval=FALSE}
# GBM hyperparameters
hyper_grid <- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(.99, 1),
  sample_rate = c(.5, .75, 1),
  col_sample_rate = c(.8, .9, 1)
)

# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  max_models = 25
  )

# build random grid search 
random_grid <- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid",
  x = X,
  y = Y,
  training_frame = train_h2o,
  hyper_params = hyper_grid,
  search_criteria = search_criteria,
  ntrees = 5000,
  stopping_metric = "RMSE",     
  stopping_rounds = 10,         
  stopping_tolerance = 0,
  nfolds = 10,
  fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE,
  seed = 123
  )
```

]]

---
# Stacking a grid search

If we look at the grid search models we see that the cross-validated RMSE range from 20756-57826
.scrollable90[
```{r gbm-grid-results, eval=FALSE}
# collect the results and sort by our model performance metric of choice
random_grid_perf <- h2o.getGrid(
  grid_id = "gbm_grid", 
  sort_by = "rmse"
  )
random_grid_perf
## H2O Grid Details
## ================
## 
## Grid ID: gbm_grid 
## Used hyper parameters: 
##   -  col_sample_rate 
##   -  learn_rate 
##   -  learn_rate_annealing 
##   -  max_depth 
##   -  min_rows 
##   -  sample_rate 
## Number of models: 25 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing rmse
##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate         model_ids               rmse
## 1             0.9       0.01                  1.0         3      1.0         1.0 gbm_grid_model_20  20756.16775065606
## 2             0.9       0.01                  1.0         5      1.0        0.75  gbm_grid_model_2 21188.696088824694
## 3             0.9        0.1                  1.0         3      1.0        0.75  gbm_grid_model_5 21203.753908665003
## 4             0.8       0.01                  1.0         5      5.0         1.0 gbm_grid_model_16 21704.257699437963
## 5             1.0        0.1                 0.99         3      1.0         1.0 gbm_grid_model_17 21710.275753497197
## 
## ---
##    col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate         model_ids               rmse
## 20             1.0       0.01                  1.0         1     10.0        0.75 gbm_grid_model_11 26164.879525289896
## 21             0.8       0.01                 0.99         3      1.0        0.75 gbm_grid_model_15  44805.63843296435
## 22             1.0       0.01                 0.99         3     10.0         1.0 gbm_grid_model_18 44854.611500840605
## 23             0.8       0.01                 0.99         1     10.0         1.0 gbm_grid_model_21 57797.874642563846
## 24             0.9       0.01                 0.99         1     10.0        0.75 gbm_grid_model_10  57809.60302408739
## 25             0.8       0.01                 0.99         1      5.0        0.75  gbm_grid_model_4  57826.30370545089
```
]

---
# Stacking a grid search

.scrollable90[
.pull-left[

Single best model applied to our test set

```{r gbm-grid-single-best, eval=FALSE}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- random_grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.performance(best_model, newdata = test_h2o)
## H2ORegressionMetrics: gbm
## 
## MSE:  466551295
## RMSE:  21599.8 #<<
## MAE:  13697.78
## RMSLE:  0.1090604
## Mean Residual Deviance :  466551295
```

]

.pull-right[

Meta learner of our grid search applied to our test set

```{r gbm-grid-super-learner, eval=FALSE}
# Train a stacked ensemble using the GBM grid
ensemble <- h2o.stackedEnsemble(
  x = X,
  y = Y,
  training_frame = train_h2o,
  model_id = "ensemble_gbm_grid",
  base_models = random_grid@model_ids,
  metalearner_algorithm = "gbm"
  )

# Eval ensemble performance on a test set
h2o.performance(ensemble, newdata = test_h2o)
## H2ORegressionMetrics: stackedensemble
## 
## MSE:  469579433
## RMSE:  21669.78 #<<
## MAE:  13499.93
## RMSLE:  0.1061244
## Mean Residual Deviance :  469579433
```

]]

.center.bold.blue[We see little benefit here; likely because our predictions are highly correlated.]

---
class: clear, center, middle, inverse

.font300.white[Auto ML Search]

---
# Auto ML search

.pull-left[

* Perform an automated search across
   - multiple base learners
   - multiple hyperparameter settings

* Frees your time

* Multiple players
   - DataRobot, a current leader in AutoML (R and Python interfaces are available).
   - H2O Driverless AI, another leader in AutoML (R and Python interfaces available).
   - auto-sklearn, an automated machine learning toolkit and a drop-in replacement for scikit-learn estimator’s (Python).

]

.pull-right[

```{r autoML-cartoon, eval=TRUE, echo=FALSE}
knitr::include_graphics("images/autoML-cartoon.png")
```

]

---
# Auto ML search

.scrollable90[
.pull-left[
* `h2o.automl()` performs automated grid search and cross validation on
   - GLMs
   - RF
   - GBM
   - XGBoost
   - Deep Learning
   - Stacked results
   
* Default will search for 1 hour but you can adjust search to run for specified:
   - time
   - number of models
   - and control individual model stopping tolerance
   
]

.pull-right[

```{r autoML-search, eval=FALSE}
auto_ml <- h2o.automl(
  x = X,
  y = Y,
  training_frame = train_h2o,
  nfolds = 5,
  max_runtime_secs = 60*60,
  max_models = 50,
  keep_cross_validation_predictions = TRUE,
  sort_metric = "RMSE",
  seed = 123
)

# assess the leader board
# get top model: auto_ml@leader
auto_ml@leaderboard
```

]]




